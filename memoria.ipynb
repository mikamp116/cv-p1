{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Esta práctica consiste en la detección de ciertas zonas de interés en coches. Está compuesta por tres apartados y en\n",
    "cada uno se deberá desarrollar un fichero. La práctica deberá ejecutarse sobre Python 3.7.X y OpenCV 4.2. y  los\n",
    "ficheros serán los siguientes:\n",
    "\n",
    "- deteccion_orb.py\n",
    "- detección_haar.py\n",
    "- deteccion_video.py.\n",
    "\n",
    "Para ejecutar la práctica deberá escribirse en la consola de comandos `python` seguido del nombre del fichero sin ningún\n",
    "otro parámetro adicional. Se supondrá que los directorios test, train y haar están en el mismo directorio que los\n",
    "ficheros python. Al ejecutar estos ficheros python se mostrará por pantalla el resultado sobre cada una de las imágenes\n",
    "o vídeos de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartado 1: Detección de coches mediante puntos de interés\n",
    "\n",
    "Este apartado aparece desarrollado en el fichero *deteccion_orb.py*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Inicialmente se importan las librerías necesarias para la ejecución del programa y se comprueba que las versiones de\n",
    "Python y OpenCV son las correctas. Se debe usar la versión 3.7 de Python y la versión 4.2 de OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "\n",
    "assert (sys.version.startswith('3.7')), \"No se esta usando la version 3.7 de Python. Version en uso: \" + sys.version\n",
    "assert (cv2.__version__.startswith('4.2')), \"No se esta usando la version 4.2 de OpenCV. Version en uso: \" + cv2.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se están usando las versiones adecuadas, no se mostrará nada por pantalla. En otro caso, se mostrará un mensaje de\n",
    "error indicando la versión que se está usando y la que se debe usar.\n",
    "\n",
    "*sys* permite comprobar la versión de Python, _os_ será útil a la hora de cargar las imágenes de un directorio,\n",
    "*cv2* se usará para cargar y procesar las imágenes, _pyplot_ permite mostrar las imágenes, *numpy* es útil para\n",
    "manipular las matrices correspondientes a las imágenes, *math* se usará para realizar operaciones aritméticas y\n",
    "trigonométricas y, por último, se medirá el rendimiento de la ejecución mediante _time_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A continuación, se definen algunos métodos para la carga de las imágenes.\n",
    "\n",
    "El método `load()` recibe una cadena de texto como parámetro que indica el nombre del directorio en el que se encuentran\n",
    "las imágenes a cargar. El directorio por defecto es _train_, ya que este método se usará siempre que se quiera entrenar\n",
    "el sistema y contiene las imágenes de entrenamiento. Este método obtiene la ruta en la que se encuentra el fichero\n",
    "actual y concatena la cadena de texto pasada como parámetro para obtener la ruta completa del directorio de\n",
    "entrenamiento. A continuación, lista los ficheros\n",
    "que se encuentran en el directorio mediante el método `os.listdir()`. Este método devuelve una lista de cadenas de texto\n",
    "con los nombres de los archivos que se encuentran en el directorio en un orden aleatorio. Para mantener el orden de las \n",
    "imágenes se llama al método `ordenar()`. Por último, se carga cada una de estas imágenes mediante el método\n",
    "`cv2.imread()`, que recibe la ruta de cada imagen del directorio y el flag 0, que indica que la imagen se carga en\n",
    "niveles de gris, y se devuelve una lista de matrices con todas las imágenes del directorio.\n",
    "\n",
    "El método `load_color()` carga las imágenes igual que el método anterior, pero manteniendo el color de  las imágenes. La\n",
    "diferencia es que en el momento de la carga no se emplea el flag 0.\n",
    "\n",
    "El método `ordenar()` recibe una lista de cadenas de texto. El método `sort()` para listas de Python ordena la lista\n",
    "del siguiente modo: \n",
    "\n",
    ">['frontal_1.jpg', 'frontal_10.jpg', 'frontal_11.jpg', ... , 'frontal_19.jpg', 'frontal_2.jpg', 'frontal_20.jpg', ...,]\n",
    "\n",
    "Pero sería más adecuado mantener las imágenes en un orden natural (0, 1, 2, 3, ...). Para ello, primero se ordenan las\n",
    "cadenas por longitud, de modo que las imágenes de los frontales del 1 al 10 quedarían al principio. Se obtienen\n",
    "estas cadenas de texto y se ordenan, se ordena el resto por separado y, por último, se concatenan y se devuelven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ordenar(lst):\n",
    "    \"\"\"Recibe una lista y la devuelve ordenada\"\"\"\n",
    "    lst.sort(key=len)\n",
    "    ret = lst[0:10]\n",
    "    ret.sort()\n",
    "    aux = lst[10:]\n",
    "    aux.sort()\n",
    "    return ret + aux\n",
    "\n",
    "\n",
    "def load(directory='train'):\n",
    "    \"\"\"Recibe el nombre de un directorio y devuelve una lista con las imagenes contenidas en el\"\"\"\n",
    "    cur_dir = os.path.abspath(os.curdir)\n",
    "    files = ordenar(os.listdir(cur_dir + '/' + directory))\n",
    "    return [cv2.imread(directory + '/' + file, 0) for file in files]\n",
    "\n",
    "\n",
    "def load_color(directory):\n",
    "    \"\"\"Recibe el nombre de un directorio y devuelve una lista con las imagenes contenidas en el a color\"\"\"\n",
    "    cur_dir = os.path.abspath(os.curdir)\n",
    "    files = ordenar(os.listdir(cur_dir + '/' + directory))\n",
    "    return [cv2.imread(directory + '/' + file) for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "También hay un método `soft_load()` que carga solamente 6 imágenes del directorio _train_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def soft_load():\n",
    "    \"\"\"Devuelve una lista con 6 imagenes preseleccionadas aleatoriamente\"\"\"\n",
    "    return [cv2.imread('train/frontal_9.jpg', 0), cv2.imread('train/frontal_39.jpg', 0),\n",
    "            cv2.imread('train/frontal_43.jpg', 0), cv2.imread('train/frontal_7.jpg', 0),\n",
    "            cv2.imread('train/frontal_19.jpg', 0), cv2.imread('train/frontal_26.jpg', 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Inicialmente se implementó otro método `load2()` para cargar las imágenes. Este método concatena una cadena de texto\n",
    "que contiene una parte común de la ruta de todas las imágenes de test con un número del 1 al 49. De este modo, es\n",
    "posible cargar las 48 imágenes de test, pero si se añaden más imágenes al directorio, estas no se cargarán.\n",
    "En cambio, el método\n",
    "`load()` sí lo haría. Por este motivo y porque ambos métodos tienen un tiempo de ejecución similar se decidió usar el\n",
    "método `load()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load2():\n",
    "    \"\"\"Devuelve una lista con las 48 imagenes de entrenamiento\"\"\"\n",
    "    return [cv2.imread('train/frontal_' + str(i) + '.jpg', 0) for i in range(1, 49)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A continuación, se implementan unos métodos `calculate_module()` y `calculate_angle_to_centre()` que serán necesarios más\n",
    "adelante para calcular el módulo del vector que une un punto con el centro de las imágenes de entrenamiento y el ángulo\n",
    "en grados que forma dicho vector respecto a la porción positiva del eje X en sentido horario, respectivamente. Ambos\n",
    "métodos reciben una tupla con las coordenadas del punto como parámetro. Pueden recibir un segundo punto para construir\n",
    "el vector. Si no lo reciben, se calcula el vector con un punto preestablecido, considerado el centro del frontal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_module(p, centre=(225, 110)):\n",
    "    \"\"\"Recibe dos puntos y devuelve el modulo del vector que los une\"\"\"\n",
    "    return np.sqrt((centre[0] - p[0]) ** 2 + (centre[1] - p[1]) ** 2)\n",
    "\n",
    "\n",
    "def calculate_angle_to_centre(p, centre=(225, 110)):\n",
    "    \"\"\"Recibe dos puntos y devuelve el angulo del vector que los une\"\"\"\n",
    "    return (math.atan2((p[1] - centre[1]), (centre[0] - p[0])) * 180 / math.pi) % 360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "También se crea una clase _Match_ para almacenar los puntos de interés aprendidos durante el entrenamiento. En esta\n",
    "clase se almacena el módulo del vector que une el punto de interés con el centro de la imagen, el ángulo que forma este\n",
    "vector como se ha indicado antes, la escala del punto de interés y el ángulo del punto de interés respecto a la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Match:\n",
    "    def __init__(self, module, kp_angle, scale, des_angle):\n",
    "        self.module = module\n",
    "        self.kp_angle = kp_angle\n",
    "        self.scale = scale\n",
    "        self.des_angle = des_angle\n",
    "\n",
    "    def get_module(self):\n",
    "        \"\"\"Devuelve el modulo del vector que une el punto de interes con el centro de la imagen\"\"\"\n",
    "        return self.module\n",
    "\n",
    "    def get_kp_angle(self):\n",
    "        \"\"\"Devuelve el angulo del vector que une el punto de interes con el centro de la imagen\"\"\"\n",
    "        return self.kp_angle\n",
    "\n",
    "    def get_scale(self):\n",
    "        \"\"\"Devuelve la escala del punto de interes\"\"\"\n",
    "        return self.scale\n",
    "\n",
    "    def get_des_angle(self):\n",
    "        \"\"\"Devuelve el angulo del punto de interes respecto de la imagen\"\"\"\n",
    "        return self.des_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un primer momento, el entrenamiento y la detección se realizaban dentro de un mismo método. Más tarde se observó que\n",
    "sería necesario utilizar estas operaciones en otros apartados, por lo que se decidió desacoplar el entrenamiento de la\n",
    "detección, implementándolos en métodos distintos.\n",
    "\n",
    "El método `train()` corresponde al entrenamiento del sistema. Este método recibe una lista de imágenes y un detector de\n",
    "puntos de interés y descriptores y devuelve una tabla de objetos *Match* con los puntos de interés aprendidos y una\n",
    "estructura de datos de tipo flann entrenada con los descriptores de las imágenes de entrenamiento. En el método se crea\n",
    "una estructura de datos tipo *FlannBasedMatcher* que sirve como árbol de búsqueda. En el constructor se indica que el \n",
    "algoritmo de búsqueda sea _LSH_ y que se busque en todas las hojas del árbol. Se inicializa la tabla de puntos de \n",
    "interés aprendidos *match_table* y se recorre la lista de imágenes recibida. Para cada imagen se obtienen los puntos\n",
    "de interés (_kps_) y descriptores (*des*).\n",
    "\n",
    "A continuación, para cada punto de interés, se almacenan en una lista auxiliar\n",
    "*image_match* los objetos _Match_ con la información necesaria para la detección. El flann es capaz de encontrar los\n",
    "descriptores más parecidos, pero además, hay que \"enseñar\" dónde está el centro de la imagen y para ello está la tabla\n",
    "*match_table*. Por cada punto de interés encontrado, se debe guardar un vector que apunte hacia el centro del frontal.\n",
    "Por ejemplo, si se encuentra el faro derecho del coche, el vector deberá apuntar a la izquierda, ya que ahí se\n",
    "encontraría el centro del frontal. De este modo, cuando se encuentre un faro derecho en una imagen de test, la tabla\n",
    "indicará que el centro está a la izquierda porque ya lo ha aprendido. Ya que las imágenes están centradas y cuadradas\n",
    "en el frontal de los coches de entrenaniento, se asumirá que los centros de los frontales corresponden a los centros de\n",
    "las imágenes. Todas las imágenes tienen el mismo tamaño y, por tanto, un centro común. Es por este motivo, que las\n",
    "funciones `calculate_module()` y `calculate_angle_to_centre()` calculan lso vectores respecto a dicho centro común, el\n",
    "punto (225,110).\n",
    "\n",
    "La lista auxiliar se introduce en la\n",
    "tabla y se añade una lista con los descriptores de la imagen al *flann* mediante el método `add()`. Es importante\n",
    "destacar que se deben añadir los descriptores de la imagen dentro de una lista, debido a que cada vez que se llama al\n",
    "método, la lista pasada como parámetro se concatena con una lista con los descriptores ya recibidos. Para que la\n",
    "búsqueda funcione correctamente y se devuelvan las distancias entre descriptores e índices correctos, se deben\n",
    "separar los descriptores de las distintas imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(images, detector):\n",
    "    \"\"\"Devuelve una tabla con los puntos de interes aprendidos y un arbol flann entrenado\"\"\"\n",
    "    FLANN_INDEX_LSH = 6\n",
    "    index_params = dict(algorithm=FLANN_INDEX_LSH, table_number=6, key_size=3, multi_probe_level=1)\n",
    "    search_params = dict(checks=-1)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    match_table = []\n",
    "    for image in images:\n",
    "        kps, des = detector.detectAndCompute(image, None)\n",
    "        image_match = [Match(calculate_module(k.pt), calculate_angle_to_centre(k.pt), k.size, k.angle) for k in kps]\n",
    "        match_table.append(image_match)\n",
    "        flann.add([des])\n",
    "\n",
    "    return match_table, flann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "El método `detect()` corresponde a la fase de detección en las imágenes de test. Recibe como\n",
    "parámetros una lista con las imágenes test, un detector de puntos de interés y descriptores, la tabla con los puntos de \n",
    "interés aprendidos, el flann entrenado, el\n",
    "número de emparejamientos a devolver y la sigma del kernel gaussiano; y devuelve una lista de tuplas con las coordenadas\n",
    "de los centros de los frontales de cada imagen de test. Adicionalmente, recibe un parámetro *debug*. Si este parámetro\n",
    "vale 1 se crearán listas para almacenar los descriptores, puntos de interés y matrices de votación de cada imagen de\n",
    "test para facilitar la depuración. Primero se define una lista *detected_points* que almacenará los puntos a\n",
    "devolver. A continuación, se\n",
    "recorre la lista de imágenes recibida y para cada imagen se obtienen los puntos de interés y los descriptores. Se\n",
    "realiza una llamada a la función `knnMatch()`, que busca los _k_ descriptores más cercanos a los de la imagen de test.\n",
    "La función devuelve los índices de la imágen de entrenamiento en la que se ha encontrado el emparejaminto\n",
    "(_imgIdx_), del descriptor de dicha imagen con el cual se ha emparejado (_trainIdx_) y del descriptor de testing\n",
    "emparejado (_queryIdx_). Una vez se tienen los emparejamientos, se recorren obteniendo los índices mencionados\n",
    "anteriomente. Es importante tener en cuenta que los descriptores de las imágenes están directamente relacionados con\n",
    "los puntos de interés a través de los índices. Esto significa que si los puntos de interés se han almacenado en orden\n",
    "correctamente según se cargaban, los índices obtenidos servirán para localizar los puntos de interés aprendidos que\n",
    "corresponden a los descriptores emparejados y que estaban almacenados en la tabla *match_table*.\n",
    "Con _imgIdx_ se accede a la fila de la tabla *match_table* en la que se encuentran los puntos de interés de la imagen\n",
    "de entrenamiento emparejada y, mediante _trainIdx_, se accede al punto de interés concreto con el que se emparejó.\n",
    "Mediante _queryIdx_ se obtiene aquel punto de interés de la imagen de test con el que se emparejó. Al localizar el\n",
    "punto de interés de entrenamiento se obtiene el objeto _Match_, el cual contiene el módulo y el ángulo del vector que,\n",
    "partiendo de las coordenadas del punto de interés, señala el punto a buscar; en este caso, el frontal del coche. Sin\n",
    "embargo, se debe tener en cuenta la relación entre las escalas y los ángulos de los puntos de interés emparejados.\n",
    "La diferencia\n",
    "entre las escalas modificará el módulo del vector para adaptarlo a la imagen de test y una simple operación aritmética\n",
    "devolverá el ángulo final del vector. Multiplicando el coseno del ángulo por el nuevo módulo y sumándo el resultado a la\n",
    "coordenada x del punto de interés de la imagen test se obtiene la coordenada x del punto en el que se encontraría el\n",
    "frontal. Ya que la\n",
    "matriz de votación tiene una resolución 10 veces menor que la imagen, se divide el resultado entre 10 y se eliminan los\n",
    "decimales (ya que la matriz solo acepta índices enteros). Se calcula el punto de la otra coordenada operando esta vez\n",
    "con el seno y se añade 1 voto a lamatriz de votación controlando que no se salga de los límites. El coseno y el seno\n",
    "calculados tienen tales signos para calcular los ángulos respecto a la porción positiva del eje x.\n",
    "\n",
    "Al terminar el bucle se dispondrá de una matriz con una celda (o varias) con valor máximo que indicará en qué celda se\n",
    "encuentra el frontal. Es posible que la matriz tenga dos zonas de valores máximos o focos, ambos con el mismo valor\n",
    "máximo, pero que uno de ellos tenga votos altos agrupados y el otro un único voto máximo. En estos casos es posible que\n",
    "se escoja el punto que no corresponde al centro, y por ello hay que tener en cuenta no solo los valores máximos\n",
    "puntuales sino la agrupación de valores altos, ya que estos últimos corresponderán al centro. Para ello, se\n",
    "convoluciona la matriz con un kernel gausiano que permitirá localizar los focos más grandes. En la matriz suavizada se\n",
    "aprecia cómo las celdas agrupadas aumentan su valor y hacen que el valor de la central destaque sobre el de la celda\n",
    "solitaria.\n",
    "\n",
    "Para obtener los índices de dicha celda se puede usar la función `unravel_index()` de Numpy. La tupla obtenida\n",
    "se multiplica por 10 para volver a la resolución inicial de la imagen de test y se añadirá a la lista declarada\n",
    "al inicio. Para finalizar, se devuelve la lista de tuplas con las coordenadas de los centros de los frontales de\n",
    "las imágenes de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def detect(images, detector, match_table, flann, knn_matches, sigma, debug):\n",
    "    \"\"\"Devuelve una lista de tuplas con las coordenadas de los puntos detectados\"\"\"\n",
    "    if debug == 1:\n",
    "        test_kps_table = []\n",
    "        test_des_table = []\n",
    "        matrices_votacion = []\n",
    "    detected_points = []\n",
    "\n",
    "    for test_image in images:\n",
    "        kps, des = detector.detectAndCompute(test_image, None)\n",
    "        if debug == 1:\n",
    "            test_des_table.append(des)\n",
    "            test_kps_table.append(kps)\n",
    "\n",
    "        results = flann.knnMatch(des, k=knn_matches)\n",
    "\n",
    "        matriz_votacion = np.zeros((int(test_image.shape[0] / 10), int(test_image.shape[1] / 10)), dtype=np.float32)\n",
    "\n",
    "        for r in results:\n",
    "            for m in r:\n",
    "                match = match_table[m.imgIdx][m.trainIdx]\n",
    "                m_test = kps[m.queryIdx]\n",
    "                trns = (m_test.size / match.get_scale()) * match.get_module()\n",
    "                angle = match.get_kp_angle() + match.get_des_angle() - m_test.angle\n",
    "                x = int((m_test.pt[0] + (trns * math.cos(angle))) / 10)\n",
    "                y = int((m_test.pt[1] - (trns * math.sin(angle))) / 10)\n",
    "                if 0 < x < matriz_votacion.shape[1] and 0 < y < matriz_votacion.shape[0]:\n",
    "                    matriz_votacion[y, x] += 1\n",
    "\n",
    "        ksize = 6 * sigma + 1\n",
    "        kernel_y = cv2.getGaussianKernel(ksize, sigma)\n",
    "        kernel_x = kernel_y.T\n",
    "        matriz_filtrada = cv2.sepFilter2D(matriz_votacion, -1, kernel_y, kernel_x)\n",
    "\n",
    "        if debug == 1:\n",
    "            matrices_votacion.append(matriz_filtrada)\n",
    "\n",
    "        z = np.unravel_index(np.argmax(matriz_filtrada, axis=None), matriz_filtrada.shape)\n",
    "        q = (int(z[1] * 10), int(z[0] * 10))\n",
    "        detected_points.append(q)\n",
    "\n",
    "    return detected_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Se ha decidido desacoplar la detección de puntos de la presentación de los resultados una vez más para permitir el uso\n",
    "de estos métodos en otros apartados. El método `draw_points()` recibe una lista de tuplas con coordenadas de puntos y\n",
    "una lista de imágenes sobre las que mostrar estos puntos. Se recorren ambas listas dibujando un círculo de color rojo,\n",
    "radio 15 y grosor 10 mediante la función `cv2.circle()` y mostrándo la imagen en pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def draw_points(images, points):\n",
    "    \"\"\"Dibuja un circulo en los puntos dados sobre las imagenes recibidas como parametro\"\"\"\n",
    "    for index in range(len(images)):\n",
    "        cv2.circle(images[index], points[index], 15, (255, 0, 0), thickness=10, lineType=8, shift=0)\n",
    "        plt.imshow(cv2.cvtColor(images[index], cv2.COLOR_RGB2BGR))\n",
    "        plt.title(\"Imagen \" + str(index + 1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Todos estos métodos se invocan en la función `main()`. Esta recibe unos parámetros que indican, respectivamente, el\n",
    "número de puntos de interés a detectar en cada imagen, el factor de escala entre los distintos niveles, y los niveles,\n",
    "de la pirámide del detector, el número de emparejamientos a calcular, la sigma del kernel gausiano y, por último, un\n",
    "entero que facilita la depuración del programa.\n",
    "\n",
    "Se comienza almacenando las imágenes de entrenamiento en *train_images* y creando un detector ORB mediante\n",
    "`cv2.ORB_create()`. El detector creará una pirámide de imágenes a distinta escala de la cual obtendrá\n",
    "los puntos de interés y los descriptores. La diferencia de escala entre cada imagen de la pirámide sera *scale_factor*\n",
    "y el número de imágenes será *pyramid_levels*, y el número de descriptores devuelto será *num_keypoints*.\n",
    "A continuación, se llama a la función de entrenamiento, que recibe los dos objetos\n",
    "creados y devuelve un árbol de búsqueda flann entrenado y una tabla con los puntos de interés aprendidos. El siguiente\n",
    "paso es cargar las imágenes de test, se cargarán en niveles de gris en una lista, y en color en otra. A continuación,\n",
    "se invoca la función de detección, que devolverá las\n",
    "coordenadas en las que se encuentran los frontales de las imágenes de test. Para acabar, se muestra la localización de\n",
    "los frontales llamando a la función `draw_points()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main(num_keypoints, scale_factor, pyramid_levels, knn_matches, gaussian_kernel_sigma, debug=0):\n",
    "    train_images = load()\n",
    "    orb = cv2.ORB_create(nfeatures=num_keypoints, scaleFactor=scale_factor, nlevels=pyramid_levels)\n",
    "    match_table, flann = train(train_images, orb)\n",
    "    test_images = load('test')\n",
    "    test_images_color = load_color('test')\n",
    "    # para hacer deteccion de una imagen en concretro, pasar esta imagen en una lista del siguiente modo\n",
    "    # test_images = [test_images[i]], donde i es el indice de la imagen a testear\n",
    "    detected_points = detect(test_images, orb, match_table, flann, knn_matches, gaussian_kernel_sigma, debug)\n",
    "    draw_points(test_images_color, detected_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Por último, se introduce una estructura de control para llamar a la función `main()` y ejecutar el código cuando se\n",
    "ejecute el mandato `python deteccion_orb.py`. De este modo, también es posible importar el fichero en otro programa e\n",
    "invocar los métodos definidos. Se han declarado unas constantes que representan todas las variables de las que depende\n",
    "el programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    NUM_KEYPOINTS = 500\n",
    "    SCALE_FACTOR = 1.3\n",
    "    PYRAMID_LEVELS = 4\n",
    "    KNN_MATCHES = 6\n",
    "    GAUSSIAN_KERNEL_SIGMA = 2\n",
    "    DEBUG = 1\n",
    "\n",
    "    # para ver las matrices de votacion, introducir el parametro DEBUG\n",
    "    #main(NUM_KEYPOINTS, SCALE_FACTOR, PYRAMID_LEVELS, KNN_MATCHES, GAUSSIAN_KERNEL_SIGMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cuando se ejecute el fichero, aparecerán las imágenes de test con los frontales marcados de uno en uno. La imagen no\n",
    "desaparecerá hasta que se cierre la ventana o se pulse la tecla *q*. En ese momento aparecerá la siguiente imagen."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Apartado 2:Detección de coches usando cv2.CascadeClassifier\n",
    "\n",
    "Este apartado aparece desarrollado en el fichero *deteccion_haar.py*."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Al inicio del fichero se importan las librerías a utilizar. Cabe destacar que se importa el fichero desarollado en\n",
    "el aparado anterior `import deteccion_orb as orbdet` para poder hacer uso del método `load()`.\n",
    "\n",
    "Al igual que en el apartado anterior, se ha decidido hacer una aplicación modular para que los métodos definidos se\n",
    "puedan usar en otros apartados."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import deteccion_orb as orbdet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para desarrollar este apartado se ha consultado la documentación de OpenCV. En ella aparecía un ejemplo de código con\n",
    "un fin muy similar al de este apartado, por lo que se decidió tomar ese código y adaptarlo."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "El método `detect()` corresponde a la fase de detección del frontal y la matrícula de los coches. Recibe una lista de\n",
    "imágenes en nivel de gris sobre las que detectar las zonas de interés, una lista con las mismas imágenes en color para\n",
    "dibujar sobre ellas las zonas de interés, clasificadores para coches y matrículas y dos valores numéricos a utilizar en\n",
    "la detección. Se declara una lista con las imágenes a devolver y\n",
    "se recorren las imágenes en gris detectando zonas de interés, en este caso frontales de coche, mediante el método\n",
    "`detectMultiScale()`. Este método recibe como parámetros la imágen sobre la que detectar, el factor\n",
    "de escala entre las imágenes generadas (similar a como hacía ORB con la pirámide) y el número mínimo de vecinos que\n",
    "debe tener una zona candidata para ser válida. La función devuelve las coordenadas del rectángulo que representa la zona\n",
    "en la que se encuentra el frontal del coche (los dos primeros enteros indican las coordenadas de la esquina superior\n",
    "izquierda y los otros dos, la anchura y altura, respectivamente). La función `cv2.rectangle()`  recibe la imagen en\n",
    "coloy y las esquinas superior izquierda e inferior derecha del rectángulo y lo dibuja con el color y grosor dados.\n",
    "La detección de la mátrícula se podría hacer de dos\n",
    "maneras: buscándola en la región delimitada por el frontal (ya que la matrícula está en él) o en toda la imagen. Se ha\n",
    "realizado la detección de ambas formas para ver el comportamiento del algoritmo. Almacenando la región en la que se\n",
    "encuentra el frontal en las variables _roi_ se busca la matrícula del mismo modo que antes y se dibuja un rectángulo\n",
    "más grueso para diferenciar la detección de matrículas en el frontal de la detección en toda la imagen. Por último, se\n",
    "busca en toda la imágen y la imagen con todas las zonas detectadas se guarda en la lista a devolver."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def detect(gray_images, color_images, coches_cascade, matriculas_cascade, scale_factor, min_neighbors):\n",
    "    \"\"\"Detecta el frontal y la matricula de los coches y devuelve una lista de imágenes con las zonas detectadas\"\"\"\n",
    "    detected_images = []\n",
    "\n",
    "    for i in range(len(gray_images)):\n",
    "        gray = gray_images[i]\n",
    "        color = color_images[i]\n",
    "\n",
    "        coche = coches_cascade.detectMultiScale(gray, scale_factor, min_neighbors)\n",
    "        for (x, y, w, h) in coche:\n",
    "            color = cv2.rectangle(color, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            roi_gray = gray[y:y + h, x:x + w]\n",
    "            roi_color = color[y:y + h, x:x + w]\n",
    "            matricula = matriculas_cascade.detectMultiScale(roi_gray, scale_factor, min_neighbors)\n",
    "            for (ex, ey, ew, eh) in matricula:\n",
    "                cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 4)\n",
    "\n",
    "        matricula = matriculas_cascade.detectMultiScale(gray, scale_factor, min_neighbors)\n",
    "        for (x, y, w, h) in matricula:\n",
    "            color = cv2.rectangle(color, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "        detected_images.append(color)\n",
    "\n",
    "    return detected_images"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "El siguiente método simplemente recibe una lista de imágenes y las muestra por pantalla indicando de qué imagen se\n",
    "trata. La función `cv2.color()` recibe una imagen y la transformación de color a realizar sobre ella. Las imágenes en\n",
    "OpenCV se cargan en BGR y lo más común es que se muestren en RGB, por lo que se cambia el orden de los canales y se\n",
    "muestra."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show(images):\n",
    "    for i in range(len(images)):\n",
    "        plt.imshow(cv2.cvtColor(images[i], cv2.COLOR_RGB2BGR))\n",
    "        plt.title(\"Imagen \" + str(i + 1))\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el directorio *haar_opencv_4.1-4.2* se incluyen los ficheros _coches.xml_ y _matriculas.xml_, que corresponden a\n",
    "clasificadores entrenados para la detección de coches y matrículas, respectivamente. Para utilizar estos clasificadores\n",
    "es necesario invocar la función `cv2.CascadeClassifier()`, que recibirá un fichero xml y devolverá un clasificador\n",
    "entrenado. En la función principal se crean dos clasificadores, uno para frontales y otro para matrículas. A\n",
    "continuación, se cargan las imágenes en niveles de gris y en color y se llama a la función de detección. Esta\n",
    "devolverá una lista de imágenes con las zonas de interés marcadas con un rectángulo. Para acabar se llama a la función\n",
    "que muestra las imágenes por pantalla."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def main(scale_factor, min_neighbors):\n",
    "    coches_cascade = cv2.CascadeClassifier('haar_opencv_4.1-4.2/coches.xml')\n",
    "    matriculas_cascade = cv2.CascadeClassifier('haar_opencv_4.1-4.2/matriculas.xml')\n",
    "\n",
    "    test_images_gray = orbdet.load('test')\n",
    "    test_images_color = orbdet.load_color('test')\n",
    "\n",
    "    frontales = detect(test_images_gray, test_images_color, coches_cascade, matriculas_cascade, scale_factor,\n",
    "                       min_neighbors)\n",
    "    show(frontales)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Al igual que en el primer apartado, las imágenes se mostrarán en ventanas separadas, que deberán cerrarse para ver la\n",
    "siguiente imagen."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Apartado 3: Detección del coche en secuencias de video"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Referencias\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}